{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 问题一\n",
    "使用PdfMiner提取PDF文档中的章节和段落信息，并打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# 读取PDF文件内容\n",
    "def extract_pdf_text(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "# 识别标题结构\n",
    "def recognize_structure(text, titles):\n",
    "    recognized_titles = [title for title in titles if re.search(title, text, re.IGNORECASE)]\n",
    "    return recognized_titles\n",
    "\n",
    "# 评估结构的逻辑完整性和条理性\n",
    "def evaluate_structure(recognized_titles, expected_titles):\n",
    "\n",
    "\n",
    "    if recognized_titles == expected_titles:\n",
    "        logic_score = 1\n",
    "    else:\n",
    "        logic_score = round(len(recognized_titles) / len(expected_titles),1)\n",
    "\n",
    "    return logic_score\n",
    "\n",
    "pdf_path = \"data/B20104870036.pdf\"  \n",
    "expected_titles = [\"摘要\", \"目录\", \"问题重述\", \"假设条件\", \"符号说明\", \"模型建立\", \"模型求解\", \"模型检验\", \"结果分析\", \"结论\", \"参考文献\", \"附录\"]\n",
    "text = extract_pdf_text(pdf_path)\n",
    "recognized_titles = recognize_structure(text, expected_titles)\n",
    "\n",
    "score = evaluate_structure(recognized_titles, expected_titles)\n",
    "print(f\"论文结构的逻辑完整性和条理性得分: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 问题二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pdfminer.high_level import extract_text\n",
    "import spacy\n",
    "import gensim\n",
    "import numpy as np\n",
    "import jieba\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import os\n",
    "\n",
    "# 加载中文模块\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "# 读取PDF文件内容\n",
    "def extract_pdf_text(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "# 将文本分成段落或句子\n",
    "def segment_text(text):\n",
    "    ...略\n",
    "    return segments\n",
    "\n",
    "# 从文本中提取赛题相关的关键词\n",
    "def extract_keywords(segments,stop_keywords):\n",
    "    ...略\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# 从文本中识别与赛题相关的主题或话题\n",
    "def evaluate_problem_statement(topics, keywords):\n",
    "    # 输出每个主题的关键词\n",
    "    topic_words = []\n",
    "    for topic in topics:\n",
    "        topic_num = topic[0]\n",
    "        topic_keywords = [word[0] for word in topic[1]]\n",
    "        topic_words.extend(topic_keywords)\n",
    "        print(f\"主题{topic_num+1}的关键词：{topic_keywords}\")\n",
    "\n",
    "    topic_coverage = len(set(keywords) & set(topic_words)) / len(keywords)\n",
    "    return round(topic_coverage,2)\n",
    "\n",
    "\n",
    "problem_pdf_path = \"data/2020华为杯B题题目.pdf\"  # 赛题题目\n",
    "paper_pdf_path = \"data/B20104870036.pdf\"    # 论文\n",
    "\n",
    "# 读取文件内容\n",
    "problem_text = extract_pdf_text(problem_pdf_path)\n",
    "paper_text = extract_pdf_text(paper_pdf_path)\n",
    "\n",
    "# 将文本分成段落或句子\n",
    "problem_segments = segment_text(problem_text)\n",
    "# 使用哈工大中文停用词库\n",
    "chinese_stopwords = [line.strip() for line in open('data/hit_stopwords.txt', encoding='utf-8').readlines()]\n",
    "\n",
    "# 去除中文停用词和符号\n",
    "filtered_paper_text = [word for word in jieba.cut(paper_text) if word not in chinese_stopwords and word.strip()]\n",
    "\n",
    "# 从文本中提取赛题相关的关键词\n",
    "problem_keywords = extract_keywords(problem_segments,chinese_stopwords)\n",
    "dict_file = 'data/custom_dict.txt'\n",
    "if not os.path.exists(dict_file):\n",
    "    # 将自定义词典列表写入文件\n",
    "    with open(dict_file, 'w', encoding='utf-8') as f:\n",
    "        for word in problem_keywords:\n",
    "            f.write(word + ' 10 n' + '\\n')  \n",
    "\n",
    "# 把题目中的关键词，加入自定义词典\n",
    "jieba.load_userdict(dict_file)\n",
    "\n",
    "# 创建并训练LDA主题模型\n",
    "num_topic = 10\n",
    "paper_dictionary = gensim.corpora.Dictionary([paper_segment.lower().split() for paper_segment in filtered_paper_text])\n",
    "paper_bow_corpus = [paper_dictionary.doc2bow(segment.lower().split()) for segment in filtered_paper_text]\n",
    "lda_model = gensim.models.LdaModel(paper_bow_corpus, id2word=paper_dictionary, num_topics=num_topic, passes=10)\n",
    "\n",
    "# 获取主题关键词\n",
    "topics = lda_model.show_topics(num_topics=num_topic, num_words=20, formatted=False)\n",
    "# 从文本中识别与赛题相关的主题或话题\n",
    "problem_statement_score = evaluate_problem_statement(topics, problem_keywords)\n",
    "print(f\"论文相关性得分: {problem_statement_score}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
